<h2>Travailler avec du code legacy</h2>

<p>Le code legacy est un code qui ne dispose pas de tests.</p>

<h3>1. Apprivoiser le code legacy</h3>

<!--TODO : graph -->
<div class="graph ">
    <div class="box-graph pink-purple">
        <p>Avoir le bon état d'esprit</p>
    </div>
    <i class="fa-solid fa-arrow-down"></i>
    <div class="box-graph pink-purple">
        <p>Réaliser un état des lieux</p>
    </div>
    <div class="box-graph pink-purple">
        <p>Présence de tests ?</p>
    </div>
    <div class="d-flex">
        <div class="box-graph pink-purple">
            <p>Enrichir les tests</p>
        </div>
        <div class="box-graph pink-purple">
            <p>Le code est-il testable ?</p>
        </div>
    </div>
</div>

<h4>1.1 Adopter le bon état d'esprit</h4>
<ul>
    <li><span class="pink-purple-txt">Garder son calme.</span></li>
    <li><span class="pink-purple-txt">Rester focus.</span>
        <p>Si l'application n'a pas besoin d'évoluer, le code existant est certainement suffisant pour tourner en
            production.</p>
        <p><span class="pink-purple-txt">Question à se poser : </span>quel objectif on poursuit ?</p>
        <ul>
            <li>Fournir une évolution ?</li>
            <li>Corriger une anomalie ?</li>
            <li>Réduire un aspect de dette technique encombrant pour des fonctionnalités à venir ?</li>
        </ul>
        <p>Le remaniement de code legacy étant risqué et onéreux, il est important de se concentrer sur les réécritures
        <span class="pink-purple-txt">qui apportent une réelle valeur.</span></p>
        <p>Rester focalisé sur un problème à la fois, procéder par petites itérations.</p>
    </li>
</ul>

<h4>1.2 S'imprégner du code</h4>
<p>Faire un état des lieux.</p>
<ul>
    <li><span class="pink-purple-txt">Le code est-il exécutable ?</span>
        <p>Quels sont les moyens d'intéragir avec l'application et d'observer les effets ?</p>
        <p>Une documentation est-elle fournie ?</p>
    </li>
    <li><span class="pink-purple-txt">Des tests sont-ils présents ?</span>
        <p>Peut-on lancer les tests ? Sont-ils annotés ignorés ou commentés ? Ce sont de vrais tests, avec assertions
            sensées ?</p>
        <p>Quand les tests peuvent-être exécutés, certains passent, d'autres non.</p>
        <p>Pour les tests qui ne passent pas, on ne peut pas en déterminer grand chose, il faudra probablement prendre
        la décision de les jeter. Ils ne sont plus en phase avec la réalité du code.</p>
    </li>
    <li><span class="pink-purple-txt">Se faire une idée générale du code.</span>
        <p>Identifier le périmètre d'intervention sur le code.</p>
        <p>Recenser les structures de données, les concepts qui sont manipulés, pour commencer à construire un modèle
        mental.</p>
        <p>Noter ce qui ne va pas, amélioration et refactoring à prévoir. S'entraîner à remarquer les patterns (des if
        qui reviennent souvent), repérer la duplication. Sans toucher au code. On peut éventuellement mettre des TODO et
        FIXME</p>
    </li>
    <li><span class="pink-purple-txt">Écrire les tests qui manquent.</span></li>
</ul>

<h3>2. Tester le code legacy</h3>

<h4>Poser un premier test</h4>
<app-callout-section title="Citation">
  <p>"Ce qui est important n'est pas ce que le code est supposé faire mais ce qu'il fait vraiment."</p>
  <p>Arnaud Lemaire <a href="https://www.youtube.com/live/LSqbXorkyfQ?si=W2TRPdEVJC4AUWbP">Youtube tutorial : kata trip
  Service.</a></p>
</app-callout-section>

<p>Contrairement au TDD, dans le code legacy, c'est au code de test de se conformer au comportement du code de production.</p>

<h4>Approche par compréhension du code</h4>
<p>Face à un code plutôt clair, on peut essayer d'identifier les règles de gestion qui serviront de fondation aux tests.</p>
<app-callout-section title="Conseil">
  <P>Tester en premier les branches les moins profondes du code (les plus directement accessibles), refactorer en premier
    les branches les plus profondes.</P>
</app-callout-section>

<pre>
  <span class="label">c#</span>
  <code class="c#">
    public List&lt;Trip&gt; getTripsByUser(User user) throws UserNotLeggedInException &#123;
      List&lt;Trip&gt; tripList = new ArrayList&lt;Trip&gt;();
      User loggedUser = UserSession.getInstance().getLoggedUser();
      boolean isFriend = false;
      if(loggedUser != null) &#123;
        // ...
      &#125; else &#123;
      throw new UserNotLoggedInException();
      &#125;
    &#125;
  </code>
</pre>

<p>On remarque que la ligne "isFriend" n'est pas utilisé à ce niveau, c'est une indication que  le scope de définition
de cette variable est certainement trop large (la déplacer à l'intérieur du bloc if serait plus pertinent).</p>

<h4>Approche par observation du code</h4>
<p>Le code est trop énigmatique, on se tourne vers une approche "boîte noire", en stimulant le code, c'est-à-dire qu'on
  le force à produire un comportement et en capturant son comportement réel.</p>
<a href="https://github.com/emilybache/GildedRose-Refactoring-Kata">Kata Gilded Rose</a>

<ul>
  <li><span class="pink-purple-txt">Construction d'un golden master</span> : Avec un code si cryptique, il est nécessaire
  de partir sur une approche contre-intuitive. Il faut se baser sur l'idée que le code tourne en production, fait ce qui
  est attendu et représente donc une vérité absolue. On va chercher à observer et tracer son comportement. Pour cela on
  va stimuler le code et enregistrer les effets produits (par exemple dans un fichier de référence). C'est cet enregistrement
  qu'on désigne par le terme de <span class="pink-purple-txt">golden master</span>.
  <p>Le golder master est comme une forme de tests de caractérisation, c'est-à-dire, des tests qui appellent le code
  existant sans le modifier pour observer comment il se comporte. Le golden master enregistre tous les résultats attendus
  pour chacun des jeux de valeur en entrée.</p>
  <p>Plus il est étoffé, plus il offre un filet de sécurité efficace contre d'éventuelles régressions.</p></li>
  <li><span class="pink-purple-txt">Choix des données de test en entrée.</span></li>
  <li><span class="pink-purple-txt">Construction itérative du golder master.</span></li>
  <li><span class="pink-purple-txt">Exhaustivité du golder master</span> : la question à se poser : à partir de quand
  on peut s'estimer confiant et arrêter d'écrire des tests ?
    <ul>
      <li><span class="pink-purple-txt">Principe de couverture de test</span> : tout nouveau test augmentant le score de
        couverture se montrera pertinent.</li>
      <li><span class="pink-purple-txt">Importance des assertions</span> : vérifier quelles sont pertinentes et s'assurer
        qu'elles échouent au moins une fois en écrivant le test (en apportant volontairement une régression sur le code testé).</li>
      <li><span class="pink-purple-txt">Couverture des tests et approche TDD</span> : les tests étant écrit avant le code,
      le code est forcément couvert par les tests.</li>
      <li><span class="pink-purple-txt">Fiabilité de la couverture de test</span> : face à un code très linéaire, il est
      facile d'obtenir une couverture à 100% avec un seul test, pourtant il ne faudrait pas avoir confiance en ce seul
      test. Il existe des techniques permettant d'éprouver davantage la robustesse des tests. En particulier, les outils
      de mutation testing utilisent des algorithmes pour modifier (provoquer des mutations) à la volée certaines lignes
      du code testé. Si au moins un test échoue, cela veut dire que le comportement de la ligne modifiée était suffisamment
      testé. Si les mutations ne provoquent aucun échec des tests : la ligne peut être soit incorrectement testée, soit
      complétement inutile.</li>
    </ul>
  </li>
  <li><span class="pink-purple-txt">Aspect fastidieux du golder master</span> : Outiller la génération du golden master :
  Au lieu d'écrire toute les assertions à la main, on peut décomposer l'écriture du golden master en deux étapes :
    <ul>
      <li>Enregistrer les comportements observés du code initial dans un fichier, qui sera inclus dans les ressources de
      test ;</li>
      <li>Exécuter le code et rechercher les valeurs attendues dans le fichier de référence avant de les appliquer sur une
      assertion générique.</li>
    </ul>
    Cette approche est asymétrique : une fois le fichier de référence construit, on pourra rejouer le code de test autant
    de fois qu'on le souhaite, à mesure que le refactoring progresse.
    Certains frameworks, <a href="https://github.com/approvals/ApprovalTests.Net">ApprovalTests</a> génère automatiquement
    l'enregistrement du golden master et son intégration des tests.
  </li>
  <li><span class="pink-purple-txt">L'exécution parallèle, une alternative du golden master</span> : Ce qui est important,
  c'est que le code refactoré continuer à avoir un code identique au code initial.
  <p>Au lieu de générer un fichier de référence, on va garder une copie du code initial, qui sera exécutée en parallèle
  du code en cours de refactoring. Les assertions de test n'auront plus qu'à vérifier à la volée que les résultats obtenus
  pour la version legacy et pour la version refactorée sont identique. De fait, c'est la version legacy du code qui devient
  le golden master.</p>
    <p>On s'assure que la version legacy du code ne soit jamais livrée en production. Pour éviter cela, on isole cette
    version legacy en la déplaçant dans le code de test. Cette approche est plus facile à mettre en place lorsque le code
     à refactorer est très regroupé et ne s'étale pas sur plusieurs composants; dans le cas contraire, on est obligé de
    dupliqué de nombreux composants, ce qui multiplie les risques d'erreurs d'inattention et de confusion, et complexifie la
    configuration des tests.</p>
  </li>
  <li><span class="pink-purple-txt">Le caractère éphémère du golden master</span> : il est juste présent le temps de
  procéder au refactoring du code. Dès que possible, il faut le remplacer par des tests qui représentent réellement le
  comportement du métier.</li>
</ul>

<h4>Les tests en tant que filet de sécurité</h4>

<p>Parfois, il est nécessaire au préalable de réorganiser le code de production.</p>



<h3>3. Rendre testable le code legacy</h3>

<p>Les problématiques :</p>
<ul>
  <li>Le code dépend d'un accès à une ressource externe;</li>
  <li>Le code dépend d'une certaine configuration de l'environnement;</li>
  <li>Le code dépend de conditions changeantes et difficile à prévoir;</li>
  <li>Le comportement du code n'est pas facilement observable au capturable.</li>
</ul>

<h4>3.1 Dépendances retorses</h4>
<p>Elles entrainent des configurations compliquées voire impossibles.</p>
<p>Les dépendances gênent fortement le maintien des caractéristiques FIRST des tests.</p>
<p>Il est donc indispensable de minimiser l'impact des dépendances sur les tests.</p>

<app-callout-section title="Remarque">
<p>Si le test est répétable dans un environnement local, c'est déjà une bonne chose. C'est un point de départ acceptable
  pour commencer le refactoring, à condition d'atteindre une couverture correcte. Il faudra par la suite s'astreindre à
faire le nécessaire pour que les tests respectent les caractéristiques FIRST.</p>
</app-callout-section>

<ul>
  <li><span class="pink-purple-txt">Identification des dépendances</span> :
    <ul>
      <li><span class="pink-purple-txt">Dépendance vers une ressource externe</span> : système extérieur avec lequel notre
      application intéragit. (BDD, gestionnaire d'authentification, service distant). En général, on travaille sur des
      copies de tests des dépendances, parfois sous forme de conteneurs, mais leurs mise en place peut se montrer assez
      lourde. On manque généralement de maîtrise sur leur comportement dans la mesure où ces copies peuvent évoluer dans le
      temps, avec les données qu'elles contiennent, et peuvent faire échouer des tests de manière imprévue ;</li>
      <li><span class="pink-purple-txt">Dépendances multiples.</span></li>
      <li><span class="pink-purple-txt">Dépendances vers des calculs non déterministes </span>: il s'agit essentiellement
      des appels à des fonctions dont le retour peut varier, même si les paramètres d'entrée ne changent pas. (Récupération
      de la date et heure, l'accès à des données pouvant être mises en cache, car le comportement du premier appel sera
        différent des suivants.</li>
      <li><span class="pink-purple-txt">Dépendance vers un état global partagé </span>: l'état d'un singleton survit au test
      qui l'utilise, ce qui fait que n'importe quel test en dépendant a un impact potentiel sur les tests suivants : le
      comportement peut être différent selon l'ordre dans lesquels ils s'exécutent, et on perd le côté indépendant de
      FIRST. Cette problématique n'est pas spécifique au singleton et se rencontre dès lors qu'on manipule un état global
      partagé entre différents composants.</li>
    </ul>

  <li><span class="pink-purple-txt">Isolation des dépendances </span>: une fois la dépendance repérée, on peut chercher
      à minimiser son existence en l'isolant. L'idée est d'introduire une abstraction entre le code à tester et la dépendance,
      pour faire tampon entre notre code et la dépendance réelle. Ainsi, la dépendance est repoussée en marge du code à
      tester, ce qui limite fortement ses effets néfastes sur notre capacité à tester.</li>
      <pre>
        <span class="label">C#</span>
        <code class="C#">
          User loggedUser = UserSession.getInstance().getLoggedUser();

          devient :

          User loggedUser = getLoggedInUser();

          avec la méthode :

          protected User getLoggedInUser()&#123;
            return UserSession.getInstance().getLoggedUser();
          &#125;
        </code>
      </pre>

    <p>Autre possibilité, introduire la valeur fournie par la dépendance sous forme de paramètre</p>
    <pre>
      <span class="label">C#</span>
      <code class="C#">
        public List&lt;Trip&gt; getTripsByUser (User user) throws UserNotLoggedInException &#123;
        &#125;

        Peut se réécrire

        public List&lt;Trip&gt; getTripsByUser (User user, User loggedUser) throws UserNotLoggedInException &#123;
        &#125;

        Ceci permet de supprimer la ligne suivante :

        User loggedUser = UserSession.getInstance().getLoggedUser();

      </code>
    </pre>
    On ne se contente pas de déplacer le problème, ces transformations offrent des possibilités nouvelles pour écrire
    les tests.
  <ul>
    <li><span class="pink-purple-txt">Les seams </span>: lorsqu'il n'existe pas d'autre possibilité viable, on peut s'octroyer
      la permission d'effectuer les modifications minimales sur le code de production pour le rendre testable. Les endroits
      sur lesquels on peut intervenir sont ceux que Michael Feathers appelle des seams
      <app-callout-section title="Michael Feathers">
        A Seam is a place to alter program behavior without changing the code.
      </app-callout-section>
      On cherche à identifier des coutures dans le code, c'est-à-dire des points précis où on peut intervenir sans modifier
      significativement le code de production.
      <p>Dans l'exemple avec le Singleton, le seam, c'est-à-dire le point d'incision, réside dans la possibilité d'extraire
        l'appel statique dans un appel non statique par extraction de méthode. Cette modification mineure offre la possibilité
        de surcharger cette méthode dans un fake de test, et donc de ne plus être embêté par cet appel singleton.</p>
      <p>Une autre approche consiste à introduire un nouveau paramètre dans la méthode appelant le singleton : ainsi on
        déplace l'appel au singleton vers une méthode de plus haut niveau, et on définit directement la valeur souhaitée dans
        le test. Cette approche est plus impactante dans la mesure où elle concerne tous les appelants de la méthode
        que l'on souhaite tester, et est donc à manier avec davantage de précautions.</p>
    </li>
  </ul>
  <li><span class="pink-purple-txt">Substitution des dépendances dans les tests </span>: le travail d'isolation des
  dépendances donne beaucoup plus d'autonomie et de possibilités pour l'écriture des tests, en particulier dans la mise
  en oeuvre de la panoplie de doublure de tests. Ces derniers vont permettre de contrôler le comportement des dépendances
  dans chacun des tests.
  <p>Si la dépendance a été isolée sous forme de paramètre, on peut aisément fournir la valeur souhaitée depuis le code
  de test.</p>
  <p>Dans le cas où une méthode a été extraite, on peut s'appuyer sur les tests doubles, en proposant par exemple un stub,
  c'est-à-dire une classe dédiée aux tests proposant une implémentation factice pour la méthode extraite : </p>
    <pre>
      <span class="label">C#</span>
      <code class="C#">
        private class TestableTripService extends TripService &#123;

          &commat;Override
          protected User getLeggedInUser()&#123;
            return loggedInUser;
          &#125;
        &#125;
      </code>
    </pre>
Cette implémentation est ensuite utilisée par le test plutôt que l'implémentation réelle se TripService.
  </li>
  <li><span class="pink-purple-txt">Injection de dépendance </span>: lorsque la couverture de tests devient satisfaisante,
  il convient de gérer les dépendances plus proprement. Encapsuler les dépendances extraites dans des composants injectables
  (en se basant sur le principe d'inversion de dépendance) est une bonne solution pour y parvenir.</li>
</ul>

<h4>3.2 Effets difficilement observables</h4>

<p>Il arrive que les effets produits par le code que nous souhaitons tester ne soient pas facilement accessibles
ou observables.</p>
<ul>
  <li>Ecriture dans une base de données;</li>
  <li>Envoi de messages asynchrones sur un bus d'évènements;</li>
  <li>Gestion d'interface graphique; </li>
  <li>Modification d'un état interne non accessible (comme l'état interne d'un objet non exposé publiquement);</li>
  <li>Ecriture dans la sortie standard.</li>
</ul>
<p>Dans ces cas, on a des effets de bord provoqués par le code : les fonctions testée modifient autre chose que les paramètres
d'entrée et ne renvoient pas de valeur.</p>
<p>Il faut donc trouver un moyen d'intercepter la modification et de récupérer la valeur écrite (qui servira dans les
assertions). Une possibilité consiste à introduire une abstraction  en façade de l'effet de bord.</p>
<p>Exemple : Kata Trivia Game</p>

<pre>
      <span class="label">C#</span>
      <code class="C#">
        purses[currentPlayer]++;
        System.out.prinln(players.get(currentPlayer)
          + "now has "
          + purses[currentPlayer]
          + "Gold Coins."
      </code>
    </pre>

<p>Cet effet, visible dans la console, n'est pas vraiment exploitable dans les tests unitaires.</p>
<p>Néanmoins, on peut introduire une abstraction, GameLog, dédiée à recevoir, stocker et restituer à la demande tous
les évènements de la partie.</p>
<p>On apporte également un peu de métier en représentant la notion de joueur et de pièce d'or de façon plus concrète :</p>

<pre>
      <span class="label">C#</span>
      <code class="C#">
        gameLog.addEvent(new PlayerEarnedGoldCoinEvent(player));
      </code>
    </pre>

<p>En tant qu'abstraction nouvellement introduite sous forme de dépendance maîtrisée, l'objet gameLog est plus facilement
accessible, mockable et vérifiable.</p>

<h3>4. Refactorer le code legacy</h3>

<h4>4.1 Se méfier des optimums locaux</h4>

<p>Quand on observe du code legacy, il est fréquent de voir beaucoup de duplication, ou encore des blocs de code qui pourraient
être extraits dans des méthodes ou des classes dédiées.</p>
<p>Il est tentant de réduire la duplication prématurément, ou de découper trop vite. Le risque est de se couper des opportunités
pour faire apparaître de la duplication à plus haut niveau.</p>

<h4>4.2 Accepter de dégrader temporairement le code</h4>

<p>Il est préférable de chercher à augmenter la duplication pour mieux faire apparaître les structures communes, quitte
à faire augmenter le volume de code. Souvent la duplication est déjà là, mais pas suffisamment explicite. Pour la mettre
en évidence :</p>
<ul>
  <li>Nommer le formatage du code (par exemple mettre systématiquement des accolades pour les instructions conditionnelles);</li>
  <li>Désextraire certaines portions de codes (certains refactorings ont pu être faits trop tôt dans la version initial du code);</li>
  <li>Réduire la complexité des conditions.</li>
</ul>

<h4>4.3 Commencer par les branches les plus profondes</h4>

<p>Les branches profondes vont représenter un volume de code plus réduit, et pourront normalement être extraites plus facilement.</p>

<h4>4.4 Savoir s'arrêter</h4>

<p>Ne pas rechercher la réécriture parfaite, on obtient à minima : </p>
<ul>
  <li>Un code plus lisible, avec des concepts et des abstractions claires et bien identifiées;</li>
  <li>Un code plus évolutif et maintenable;</li>
  <li>La présence de tests fournissant un filet de sécurité.</li>
</ul>

<h3>5. L'approche Scratch refactoring</h3>
 <p>Pour des systèmes hérités vastes ou d'une certaine complexité, il est difficile d'anticiper les étapes à suivre pour
 améliorer le code.</p>
<p>L'approche de <span class="pink-purple-txt">scratch refactoring</span>, décrite pas Michael Feathers dans son livre
<span class="pink-purple-txt">Working Effectively with Legacy Code</span> consiste à modifier le code avec une certaine
liberté pour esquisser vite les étapes de façon concrète, tout en sachant que <span class="pink-purple-txt">le résultat ne sera jamais livré en
  production</span>. Cela permet d'avancer sans se soucier d'introduire des régressions, à seule fin de mieux se projeter
dans le travail nécessaire.</p>
<p>En pratique le scratch refactoring se termine par la prise de notes des étapes nécessaires, qui deviendront des tâches
ultérieures, avant d'annuler les modifications (ou de les garder sur une branche dédiée du contrôle de sources qui n'est
pas destinée à être livrée).</p>

<h3>6. L'approche d'amélioration marginale</h3>

<p>Il est facile de se décourager face à un système hérité de grande taille et de qualité faible, en réalisant qu'il ne
sera jamais possible de le réparer entièrement.</p>
<p>Pourtant, il reste possible d'améliorer son quotidien avec le principe "à partir de maintenant on fait mieux"</p>

